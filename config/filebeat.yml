output.elasticsearch:
  hosts: '${ELASTICSEARCH_HOSTS:elasticsearch:9200}'
  
setup.kibana.host:
  hosts: '${KIBANA_HOSTS:kibana:5601}'

filebeat.modules:

#-------------------------------- System Module --------------------------------

- module: system
  syslog:
    enabled: true

#-------------------------------- Apache Module --------------------------------

- module: apache
  access:
    enabled: true

  error:
    enabled: true

#---------------------------- Elasticsearch Module ----------------------------
- module: elasticsearch
  server:
    enabled: true

  gc:
    enabled: true

  audit:
    enabled: true

  slowlog:
    enabled: true

  deprecation:
    enabled: true

#-------------------------------- Kibana Module --------------------------------
- module: kibana
  log:
    enabled: true

  audit:
    enabled: true

#------------------------------- Logstash Module -------------------------------

- module: logstash
  log:
    enabled: true

  slowlog:
    enabled: true

#-------------------------------- Nginx Module --------------------------------
#- module: nginx
  #access:
    #enabled: true

  #error:
    #enabled: true

  #ingress_controller:
  #  enabled: false

#------------------------------ PostgreSQL Module ------------------------------
#- module: postgresql
  #log:
    #enabled: true

    #input:

#-------------------------------- Redis Module --------------------------------
#- module: redis
  #log:
    #enabled: true
    #var.paths: ["/var/log/redis/redis-server.log*"]

  #slowlog:
    #enabled: true
    #var.hosts: ["localhost:6379"]
    #var.password:

#=========================== Filebeat inputs =============================

filebeat.inputs:

#------------------------------ Log input --------------------------------

- type: log
  enabled: true
  paths:
    - /var/log/*.log
  encoding: plain

  fields:
   level: debug
   review: 1

  fields_under_root: false
  keep_null: true
  publisher_pipeline.disable_host: false
  ignore_older: 0
  scan_frequency: 10s
  recursive_glob.enabled: true

  ### JSON configuration
  json.message_key:
  json.keys_under_root: false
  json.overwrite_keys: false
  json.expand_keys: false
  json.add_error_key: false

#--------------------------- Filestream input ----------------------------

- type: filestream
  id: my-filestream-id
  enabled: false
  paths:
    - /var/log/*.log

# =========================== Filebeat autodiscover ============================

# Autodiscover allows you to detect changes in the system and spawn new modules
# or inputs as they happen.

#filebeat.autodiscover:
  # List of enabled autodiscover providers
#  providers:
#    - type: docker
#      templates:
#        - condition:
#            equals.docker.container.image: busybox
#          config:
#            - type: container
#              paths:
#                - /var/log/containers/*.log

# ========================== Filebeat global options ===========================

filebeat.registry.path: ${path.data}/registry
filebeat.registry.file_permissions: 0600
filebeat.registry.flush: 30m
filebeat.overwrite_pipelines: false
filebeat.shutdown_timeout: 0

filebeat.config:
  inputs:
    enabled: false
    path: inputs.d/*.yml
    reload.enabled: true
    reload.period: 10s
  modules:
    enabled: true
    path: modules.d/*.yml
    reload.enabled: true
    reload.period: 10s

# ================================== General ===================================

name: pfr
tags: ["pfr"]

fields:
  env: develop

fields_under_root: false
timestamp.precision: millisecond

# ================================= Processors =================================

# Processors are used to reduce the number of fields in the exported event or to
# enhance the event with external metadata. This section defines a list of
# processors that are applied one by one and the first one receives the initial
# event:
#
#   event -> filter1 -> event1 -> filter2 ->event2 ...
#
# The supported processors are drop_fields, drop_event, include_fields,
# decode_json_fields, and add_cloud_metadata.
#
# For example, you can use the following processors to keep the fields that
# contain CPU load percentages, but remove the fields that contain CPU ticks
# values:
#
#processors:
#  - include_fields:
#      fields: ["cpu"]
#  - drop_fields:
#      fields: ["cpu.user", "cpu.system"]
#
# The following example drops the events that have the HTTP response code 200:
#
#processors:
#  - drop_event:
#      when:
#        equals:
#          http.code: 200
#
# The following example renames the field a to b:
#
#processors:
#  - rename:
#      fields:
#        - from: "a"
#          to: "b"
#
# The following example tokenizes the string into fields:
#
#processors:
#  - dissect:
#      tokenizer: "%{key1} - %{key2}"
#      field: "message"
#      target_prefix: "dissect"
#
# The following example enriches each event with metadata from the cloud
# provider about the host machine. It works on EC2, GCE, DigitalOcean,
# Tencent Cloud, and Alibaba Cloud.
#
#processors:
#  - add_cloud_metadata: ~
#
# The following example enriches each event with the machine's local time zone
# offset from UTC.
#
#processors:
#  - add_locale:
#      format: offset
#
# The following example enriches each event with docker metadata, it matches
# given fields to an existing container id and adds info from that container:
#
#processors:
#  - add_docker_metadata:
#      host: "unix:///var/run/docker.sock"
#      match_fields: ["system.process.cgroup.id"]
#      match_pids: ["process.pid", "process.parent.pid"]
#      match_source: true
#      match_source_index: 4
#      match_short_id: false
#      cleanup_timeout: 60
#      labels.dedot: false
#      # To connect to Docker over TLS you must specify a client and CA certificate.
#      #ssl:
#      #  certificate_authority: "/etc/pki/root/ca.pem"
#      #  certificate:           "/etc/pki/client/cert.pem"
#      #  key:                   "/etc/pki/client/cert.key"
#
# The following example enriches each event with docker metadata, it matches
# container id from log path available in `source` field (by default it expects
# it to be /var/lib/docker/containers/*/*.log).
#
#processors:
#  - add_docker_metadata: ~
#
# The following example enriches each event with host metadata.
#
#processors:
#  - add_host_metadata: ~
#
# The following example enriches each event with process metadata using
# process IDs included in the event.
#
#processors:
#  - add_process_metadata:
#      match_pids: ["system.process.ppid"]
#      target: system.process.parent
#
# The following example decodes fields containing JSON strings
# and replaces the strings with valid JSON objects.
#
#processors:
#  - decode_json_fields:
#      fields: ["field1", "field2", ...]
#      process_array: false
#      max_depth: 1
#      target: ""
#      overwrite_keys: false
#
#processors:
#  - decompress_gzip_field:
#      from: "field1"
#      to: "field2"
#      ignore_missing: false
#      fail_on_error: true
#
# The following example copies the value of message to message_copied
#
#processors:
#  - copy_fields:
#      fields:
#        - from: message
#          to: message_copied
#      fail_on_error: true
#      ignore_missing: false
#
# The following example truncates the value of message to 1024 bytes
#
#processors:
#  - truncate_fields:
#      fields:
#        - message
#      max_bytes: 1024
#      fail_on_error: false
#      ignore_missing: true
#
# The following example preserves the raw message under event.original
#
#processors:
#  - copy_fields:
#      fields:
#        - from: message
#          to: event.original
#      fail_on_error: false
#      ignore_missing: true
#  - truncate_fields:
#      fields:
#        - event.original
#      max_bytes: 1024
#      fail_on_error: false
#      ignore_missing: true
#
# The following example URL-decodes the value of field1 to field2
#
#processors:
#  - urldecode:
#      fields:
#        - from: "field1"
#          to: "field2"
#      ignore_missing: false
#      fail_on_error: true